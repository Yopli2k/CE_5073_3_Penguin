{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importación de los datos",
   "id": "511250625671697a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T23:01:52.040215Z",
     "start_time": "2024-12-06T23:01:52.033532Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv('../datasets/penguins_size.txt')\n",
    "print(penguins.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
      "0  Adelie  Torgersen              39.1             18.7                181   \n",
      "1  Adelie  Torgersen              39.5             17.4                186   \n",
      "2  Adelie  Torgersen              40.3             18.0                195   \n",
      "3  Adelie  Torgersen              36.7             19.3                193   \n",
      "4  Adelie  Torgersen              39.3             20.6                190   \n",
      "\n",
      "   body_mass_g     sex  \n",
      "0         3750    MALE  \n",
      "1         3800  FEMALE  \n",
      "2         3250  FEMALE  \n",
      "3         3450  FEMALE  \n",
      "4         3650    MALE  \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Separación de datos: 80% train, 20% test",
   "id": "148ec0d00b195ab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T23:01:55.270410Z",
     "start_time": "2024-12-06T23:01:55.265770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = penguins.drop(columns=[\"species\"])  # Characteristics without species\n",
    "y = penguins[\"species\"]                 # Label species\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42, stratify = y\n",
    ")"
   ],
   "id": "412d9feb902e0a7f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Estandarización\n",
    "## One-Hot con DictVectorizer\n",
    "## Normalización con StandardScaler\n",
    "## Preparación de la columna objetivo"
   ],
   "id": "30c563b1d790d613"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T23:01:58.364880Z",
     "start_time": "2024-12-06T23:01:58.354982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Separate categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = X_train.select_dtypes(exclude=[\"object\"]).columns\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X_train_dict = X_train[categorical_cols].to_dict(orient=\"records\")\n",
    "X_test_dict = X_test[categorical_cols].to_dict(orient=\"records\")\n",
    "X_train_transform = vectorizer.fit_transform(X_train_dict)\n",
    "X_test_transform = vectorizer.transform(X_test_dict)\n",
    "\n",
    "# Convert categorical columns for vectorizer\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_std = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Combine transformed data\n",
    "X_train_prepared = np.hstack((X_train_std, X_train_transform))\n",
    "X_test_prepared = np.hstack((X_test_std, X_test_transform))\n",
    "\n",
    "# Normalize target column\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ],
   "id": "1e0932140b8fe455",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entrenamiento de los modelos\n",
    "## Logistic Regression\n",
    "### Creamos el modelo, lo entrenamos y lo serializamos"
   ],
   "id": "4751139e08eca4c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T23:02:04.167023Z",
     "start_time": "2024-12-06T23:02:04.160562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "logistic_model.fit(X_train_prepared, y_train_encoded)\n",
    "\n",
    "with open('../models/logistic.pck', 'wb') as file:\n",
    "    pickle.dump((logistic_model, scaler, vectorizer), file)"
   ],
   "id": "5e384bae12efec5b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SVM\n",
    "### Creamos el modelo, lo entrenamos y lo serializamos"
   ],
   "id": "b05b46b04a625cc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T23:02:07.197081Z",
     "start_time": "2024-12-06T23:02:07.191833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(\n",
    "    kernel='linear',\n",
    "    C=1.0,\n",
    "    random_state=42,\n",
    "    probability=True\n",
    ")\n",
    "svm_model.fit(X_train_prepared, y_train_encoded)\n",
    "\n",
    "with open('../models/svm.pck', 'wb') as file:\n",
    "    pickle.dump((svm_model, scaler, vectorizer), file)"
   ],
   "id": "b78f889aef5fef9d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision Trees\n",
    "### Creamos el modelo, lo entrenamos y lo serializamos"
   ],
   "id": "128c1c75f82633"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T23:02:10.554234Z",
     "start_time": "2024-12-06T23:02:10.550036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree_model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "dtree_model.fit(X_train_prepared, y_train_encoded)\n",
    "\n",
    "with open('../models/dtree.pck', 'wb') as file:\n",
    "    pickle.dump((dtree_model, scaler, vectorizer), file)"
   ],
   "id": "8b63e855f5c5d932",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KNN\n",
    "### Creamos el modelo, lo entrenamos y lo serializamos"
   ],
   "id": "c11d131d08c58b2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T23:02:14.206764Z",
     "start_time": "2024-12-06T23:02:14.202534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=3,\n",
    "    weights='uniform',\n",
    "    p=2,\n",
    "    metric='minkowski'\n",
    ")\n",
    "knn_model.fit(X_train_prepared, y_train_encoded)\n",
    "\n",
    "with open('../models/knn.pck', 'wb') as file:\n",
    "    pickle.dump((knn_model, scaler, vectorizer), file)"
   ],
   "id": "367715af14e655c3",
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
